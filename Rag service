# rag_service.py
"""
Reconstructed from OCR dump — cleaned and made syntactically valid Python.
Preserves async parallel embedding flow + sync parallel fallback.
Review lines marked with `# FIXME` — they indicate uncertain reconstructions.
"""

from __future__ import annotations

import asyncio
import concurrent.futures
import os
import random
import ssl
import time
from itertools import islice
from typing import Any, Dict, Iterable, List, Optional, Tuple

import aiohttp
import faiss
import numpy as np
import requests

# Project imports (may need adjustment depending on real project layout)
from backend.core.config import CERT_FILE, gpt_40_config, embedding_config  # FIXME: confirm names
from backend.integrations.llm_client import LLMClient
from backend.integrations.confluence_client import ConfluenceClient
from backend.prompts import rag_prompt
from backend.services.confluence_chunker import (
    chunk_confluence_page,
    semantic_chunk_confluence_page,
)
from backend.services.keyword_service import decompose_keywords, normalize_keywords  # FIXME: used functions
from backend.services.page_service import fetch_all_confluence_pages
from backend.services.rag_state import qa_sessions
from backend.services.search_service import get_confluence_base_url, search_confluence_pages

# Constants (inferred and cleaned)
BATCH_SIZE = 50
MAX_WORKERS = 4
MAX_RETRIES = 4  # Lowered for faster fail (as original comment suggested)
DELAY = 1  # Lowered for faster retry
MAX_BACKOFF = 10  # Cap backoff to 10 seconds
EMBED_EPS = 1e-8


# -------------------------
# Utilities
# -------------------------
def batched(iterable: Iterable, n: int):
    """Yield successive n-sized batches from iterable."""
    it = iter(iterable)
    while batch := list(islice(it, n)):
        yield batch


# -------------------------
# Async embedding functions
# -------------------------
async def get_embeddings_batch_async(
    texts: List[str],
    max_retries: int = MAX_RETRIES,
    delay: float = DELAY,
) -> List[np.ndarray]:
    """
    Call Embedding API asynchronously and return list of numpy arrays.
    Uses aiohttp and retry/backoff for 429 or transient errors.
    """
    if not texts:
        return []

    # Build API URL and headers from config
    api_base = gpt_40_config.get("api_base", "").replace("https://", "").rstrip("/")
    deployment = embedding_config.get("deployment")  # FIXME: ensure embedding_config name
    api_version = gpt_40_config.get("api_version")
    api_key = gpt_40_config.get("api_key")

    url = (
        f"https://{api_base}/openai/deployments/{deployment}/embeddings?api-version={api_version}"
    )
    headers = {"Content-Type": "application/json", "api-key": api_key}
    payload = {"input": texts}

    ca_bundle = os.environ.get("REQUESTS_CA_BUNDLE")
    ssl_context = ssl.create_default_context(cafile=ca_bundle) if ca_bundle else None

    for attempt in range(max_retries):
        try:
            timeout = aiohttp.ClientTimeout(total=60)
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    url, headers=headers, json=payload, ssl=ssl_context, timeout=timeout
                ) as response:
                    # handle rate limit
                    if response.status == 429 and attempt < max_retries - 1:
                        wait_time = min(delay * (2 ** attempt) + random.uniform(0, 2), MAX_BACKOFF)
                        print(
                            f"[WARN] Rate limit hit. Retrying in {wait_time:.1f}s... "
                            f"(Attempt {attempt+1}/{max_retries})"
                        )
                        await asyncio.sleep(wait_time)
                        continue

                    response.raise_for_status()
                    resp_json = await response.json()
                    return [np.array(d["embedding"]) for d in resp_json.get("data", [])]
        except Exception as e:
            if attempt < max_retries - 1:
                wait_time = min(delay * (2 ** attempt) + random.uniform(0, 2), MAX_BACKOFF)
                print(
                    f"[WARN] Failed to get embedding: {e}. Retrying in {wait_time:.1f}s..."
                    f" (Attempt {attempt+1}/{max_retries})"
                )
                await asyncio.sleep(wait_time)
                continue
            else:
                print(f"[ERROR] Failed to get embedding after {max_retries} attempts: {e}")
                raise


async def parallel_embeddings_async(
    chunks: List[str], max_concurrent_batches: int = 2
) -> List[np.ndarray]:
    """
    Split chunks into batches and embed them asynchronously with concurrency limits.
    Returns a list of normalized numpy embeddings (float32).
    """
    if not chunks:
        return []

    batch_list = list(batched(chunks, BATCH_SIZE))

    semaphore = asyncio.Semaphore(max_concurrent_batches)

    async def sem_task(batch: List[str]) -> List[np.ndarray]:
        async with semaphore:
            return await get_embeddings_batch_async(batch)

    tasks = [asyncio.create_task(sem_task(batch)) for batch in batch_list]
    embeddings: List[np.ndarray] = []

    for i, task in enumerate(tasks):
        try:
            result = await task
            embeddings.extend(result)
        except Exception as e:
            # Stop further processing on first failure (mirrors original comment)
            print(f"[WARN] Embedding batch {i+1} failed: {e}. Stopping further batches.")
            break

    # Normalize embeddings (avoid division by zero)
    normed = []
    for e in embeddings:
        norm = np.linalg.norm(e)
        if norm > EMBED_EPS:
            normed.append((e / norm).astype("float32"))
        else:
            normed.append(e.astype("float32"))
    return normed


# -------------------------
# Synchronous (legacy) embedding functions
# -------------------------
def get_embeddings_batch(
    texts: List[str], max_retries: int = MAX_RETRIES, delay: float = DELAY
) -> List[np.ndarray]:
    """
    Synchronous embedding call using requests (legacy fallback).
    """
    if not texts:
        return []

    api_base = gpt_40_config.get("api_base", "").replace("https://", "").rstrip("/")
    deployment = embedding_config.get("deployment")  # FIXME: confirm
    api_version = gpt_40_config.get("api_version")
    api_key = gpt_40_config.get("api_key")

    url = f"https://{api_base}/openai/deployments/{deployment}/embeddings?api-version={api_version}"
    headers = {"Content-Type": "application/json", "api-key": api_key}
    payload = {"input": texts}

    for attempt in range(max_retries):
        try:
            resp = requests.post(
                url,
                headers=headers,
                json=payload,
                verify=os.environ.get("REQUESTS_CA_BUNDLE"),
                timeout=60,
            )
            # handle HTTP errors including 429
            if resp.status_code == 429 and attempt < max_retries - 1:
                wait_time = min(delay * (2 ** attempt) + random.uniform(0, 2), MAX_BACKOFF)
                print(f"[WARN] Rate limit. Retrying in {wait_time:.1f}s...")
                time.sleep(wait_time)
                continue
            resp.raise_for_status()
            return [np.array(d["embedding"]) for d in resp.json().get("data", [])]
        except requests.exceptions.HTTPError as e:
            # If it's a recoverable error and we still have attempts left, retry
            if resp is not None and resp.status_code == 429 and attempt < max_retries - 1:
                wait_time = min(delay * (2 ** attempt) + random.uniform(0, 2), MAX_BACKOFF)
                time.sleep(wait_time)
                continue
            raise
        except Exception:
            if attempt < max_retries - 1:
                wait_time = min(delay * (2 ** attempt) + random.uniform(0, 2), MAX_BACKOFF)
                time.sleep(wait_time)
                continue
            raise


def parallel_embeddings_sync(chunks: List[str], max_workers: int = MAX_WORKERS) -> List[np.ndarray]:
    """
    Parallel synchronous embedding: splits into batches and uses ThreadPoolExecutor to run
    get_embeddings_batch concurrently. Returns normalized embeddings.
    """
    if not chunks:
        return []

    batch_list = list(batched(chunks, BATCH_SIZE))
    embeddings: List[np.ndarray] = []

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(get_embeddings_batch, batch) for batch in batch_list]
        for i, fut in enumerate(concurrent.futures.as_completed(futures)):
            try:
                res = fut.result()
                embeddings.extend(res)
            except Exception as e:
                print(f"[WARN] Sync embedding batch {i+1} failed: {e}. Continuing with others.")
                # depending on policy, could break here; we continue to collect what we can

    # Normalize
    normed = []
    for e in embeddings:
        norm = np.linalg.norm(e)
        if norm > EMBED_EPS:
            normed.append((e / norm).astype("float32"))
        else:
            normed.append(e.astype("float32"))
    return normed


# -------------------------
# LLM helper
# -------------------------
def get_chat_completion(context: str, question: str) -> str:
    """
    Wrapper around LLMClient to get chat completion.
    """
    llm_client = LLMClient()
    return llm_client.chat_completion(context, question)


# -------------------------
# Confluence RAG class
# -------------------------
class ConfluenceQA:
    def __init__(self, pages_json: List[Dict[str, Any]], *, use_async_embeddings: bool = True):
        """
        Build chunks from pages_json, create embeddings and FAISS index.
        pages_json: list of page dicts as returned by fetch_all_confluence_pages
        """
        self.chunks: List[Dict[str, Any]] = []
        for page in pages_json:
            # semantic_chunk_confluence_page should return a list of chunks with fields
            page_chunks = semantic_chunk_confluence_page(page)
            self.chunks.extend(page_chunks)

        print(f"[INFO] Chunking completed. Total chunks: {len(self.chunks)}")
        if not self.chunks:
            raise ValueError("Cannot build RAG knowledge base with empty text chunks.")

        # Prepare texts for embedding
        texts = [chunk.get("text", "") for chunk in self.chunks]

        # Run embeddings (async preferred)
        if use_async_embeddings:
            try:
                self.embeddings = asyncio.run(parallel_embeddings_async(texts, max_concurrent_batches=2))
            except Exception as e:
                print(f"[WARN] Async embeddings failed: {e}. Falling back to sync embeddings.")
                self.embeddings = parallel_embeddings_sync(texts)
        else:
            self.embeddings = parallel_embeddings_sync(texts)

        print(f"[INFO] Embeddings stored in vector DB. Total embeddings: {len(self.embeddings)}")
        if not self.embeddings:
            raise ValueError("No Context Found: Embedding API failed or rate limited. Please try again later.")

        self.index = self.build_faiss_index(self.embeddings)
        print(f"[INFO] Vector DB index built. Size: {self.index.ntotal}")

    def build_faiss_index(self, embeddings: List[np.ndarray]) -> faiss.Index:
        if not embeddings:
            raise ValueError("No embeddings to build FAISS index. Knowledge base is empty.")
        arr = np.stack(embeddings).astype("float32")
        # Using inner product index with vectors normalized => cosine similarity
        index = faiss.IndexFlatIP(arr.shape[1])
        index.add(arr)
        return index

    def retrieve(self, question: str, top_k: int = 20) -> List[Tuple[Dict[str, Any], float]]:
        """
        Retrieve top_k chunks for the question.
        Uses synchronous embedding for query (legacy); we normalize query embedding.
        """
        q_emb_list = get_embeddings_batch([question])
        if not q_emb_list:
            return []
        q_emb = q_emb_list[0].astype("float32")
        norm = np.linalg.norm(q_emb)
        if norm > EMBED_EPS:
            q_emb = q_emb / norm
        q_emb = q_emb.reshape(1, -1)

        D, I = self.index.search(q_emb, top_k)
        # Ensure uniqueness and preserve order
        seen = set()
        indices = [i for i in I[0] if i not in seen and not seen.add(i)]
        results = []
        for j, i in enumerate(indices):
            score = float(D[0][j]) if D.size > 0 else 0.0
            results.append((self.chunks[i], score))
        return results

    def answer(self, question: str, max_results: int = 10) -> Tuple[str, bool, List[Tuple[Dict[str, Any], float]]]:
        """
        Form context from top retrieved chunks, call LLM, and return answer + debug info.
        """
        top_k = 80
        context_chunks_scores = self.retrieve(question, top_k=top_k)
        system_prompt = rag_prompt.RESPONSE_PROMPT  # FIXME: confirm attribute name

        def format_chunk(chunk: Dict[str, Any]) -> str:
            text = chunk.get("text", "")
            title = chunk.get("page_title", "")
            url = chunk.get("url", "")
            return f"[Title: {title}] [URL: {url}]\n{text}"

        context = system_prompt + "\n\n" + "\n\n".join([format_chunk(chunk) for chunk, _ in context_chunks_scores])

        from_vector_db = True

        # Early bailouts
        if not context_chunks_scores or not context.strip():
            return "No Context Found", False, []

        answer = get_chat_completion(context, question)
        return answer, from_vector_db, context_chunks_scores


# -------------------------
# External helpers & flows
# -------------------------
def get_ga_response(
    all_pages: List[Dict[str, Any]],
    chunk_to_url: Dict[str, str],
    question: str,
    session_id: str,
    debug: Dict[str, Any],
    base_choice: str,
) -> Tuple[Dict[str, Any], Optional[str]]:
    """
    Create QA model, store in session, ask question, and format output with cited urls and scores.
    """
    import re

    qa_model = ConfluenceQA(all_pages)
    qa_sessions[session_id] = qa_model
    answer, from_vector_db, context_chunks_scores = qa_model.answer(question)

    # Extract cited URLs from the answer using markdown link pattern
    cited_urls = set(re.findall(r'\[.*?\]\((https?://[^)]+)\)', answer))

    # Strict: only include chunk scores for cited URLs
    chunk_scores = []
    for chunk, score in context_chunks_scores:
        if cited_urls:
            url = chunk_to_url.get(chunk.get("text", ""), "")
            if url and url in cited_urls:
                chunk_scores.append({"url": url, "score": round(score, 2)})

    max_score = (
        round(max([score for chunk, score in context_chunks_scores if chunk_to_url.get(chunk.get("text", ""), "") in cited_urls]), 2)
        if chunk_scores
        else 0.0
    )

    response = {
        "answer": answer,
        "from_vector_db": from_vector_db,
        "context_chunks_scores": chunk_scores,
        "max_score": max_score,
        "session_id": session_id,
        "debug": debug,
    }
    return response, None


def run_keyword_rag(question: str, base_choice: str, session_id: str, debug: Dict[str, Any]):
    """
    Complete keyword RAG flow: extract keywords, search confluence, fetch pages,
    build QA model and return final response dict.
    """
    keywords = normalize_keywords(question)
    debug["keywords"] = keywords
    debug["base_choice"] = base_choice

    search_results = search_confluence_pages(keywords, source=base_choice)
    all_urls = [entry["url_by_title"] for entry in search_results if entry.get("url_by_title")]
    print(f"[INFO] URLS found: {len(all_urls)}")
    print("[INFO] URLs list:", all_urls)

    user = os.environ.get("DOMAIN_USER", "")
    password = os.environ.get("DOMAIN_PASSWORD", "")
    all_pages = fetch_all_confluence_pages(all_urls, user, password, cert_file=CERT_FILE)

    print(f"[INFO] URLS extracted: {len(all_pages)}")  # FIXME: original had slight variations

    # Quick usable context check (reconstructed from OCR)
    def has_usable_context(pages: List[Dict[str, Any]]) -> bool:
        for page_idx, page in enumerate(pages):
            chunks = page.get("chunks", []) or page.get("content", [])
            for chunk_idx, chunk in enumerate(chunks):
                if chunk.get("type") == "text" and chunk.get("content", "").strip():
                    print(f"[DEBUG] Usable text chunk found in page {page_idx}, chunk {chunk_idx}")
                    return True
                if chunk.get("type") == "table" and chunk.get("rows"):
                    print(f"[DEBUG] Usable table chunk found in page {page_idx}, chunk {chunk_idx}")
                    return True
        print("[DEBUG] has usable_context: No usable context found in any page.")
        return False

    if not has_usable_context(all_pages):
        return {"error": "No usable context found"}, None

    # Build map from chunk text to URL (approximation)
    chunk_to_url: Dict[str, str] = {}
    for page in all_pages:
        url = page.get("url", "")
        page_chunks = semantic_chunk_confluence_page(page)
        for c in page_chunks:
            chunk_to_url[c.get("text", "")] = url

    # Build QA model and answer
    response, _ = get_ga_response(all_pages, chunk_to_url, question, session_id, debug, base_choice)
    return response, None


# -------------------------
# Text normalization helpers
# -------------------------
NO_CONTEXT_PHRASES = [
    # add phrases that indicate no context found
    "No Context Found",
    "I couldn't find any reliable information in Confluence for that topic.",
]


def normalize_text(text: str) -> str:
    """Normalize text for robust comparison: lowercase, standardize quotes, strip whitespace."""
    if text is None:
        return ""
    normalized = text.lower()
    # replace various curly quotes with straight ones and remove extra whitespace
    normalized = normalized.replace("’", "'").replace("“", '"').replace("”", '"')
    normalized = normalized.replace("\u2013", "-").replace("\u2014", "-")
    return normalized.strip()


def should_force_search(answer: str) -> bool:
    """
    Returns True if the answer indicates no usable context was found.
    Heuristic: if the normalized answer contains any NO_CONTEXT_PHRASES -> don't force.
    """
    normalized_answer = normalize_text(answer)
    for phrase in NO_CONTEXT_PHRASES:
        normalized_phrase = normalize_text(phrase)
        if normalized_phrase in normalized_answer:
            return False
    return True


# -------------------------
# If run as script (debug)
# -------------------------
if __name__ == "__main__":
    # Quick smoke test / demo (won't run without project dependencies)
    print("rag_service.py loaded. This module is intended to be imported by the application.")
