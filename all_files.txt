app.py

import streamlit as st
import os
import json
import time
from dotenv import load_dotenv

# Load environment variables
load_dotenv()
# Import the ConfluenceQA class
from confluence_qa import ConfluenceQA

try:
    from hyperplane.utils import is_jhub
    if is_jhub():
        openaiKeyFile = '/root/.secret/openai_key.json'
    else:
        openaiKeyFile = '/etc/hyperplane/secrets/openai_key.json'
    with open(openaiKeyFile) as f:
        os.environ["OPENAI_API_KEY"] = json.load(f)['openai_key']
except Exception as e:
    print(e)
    load_dotenv()

st.set_page_config(
    page_title='Q&A Bot for Confluence Page',
    page_icon='⚡',
    layout='wide',
    initial_sidebar_state='auto',
)
if "config" not in st.session_state:
    st.session_state["config"] = {}
if "confluence_qa" not in st.session_state:
    st.session_state["confluence_qa"] = None

@st.cache_resource
def load_confluence(config):
    # st.write("loading the confluence page")
    confluence_qa = ConfluenceQA(config=config)
    confluence_qa.init_embeddings()
    confluence_qa.init_models()
    confluence_qa.vector_db_confluence_docs()
    confluence_qa.retreival_qa_chain()
    return confluence_qa

with st.sidebar.form(key ='Form1'):
    st.markdown('## Add your configs')
    confluence_url = st.text_input("paste the confluence URL", "https://templates.atlassian.net/wiki/")
    username = st.text_input(label="confluence username",
                             help="leave blank if confluence page is public",
                             type="password")
    space_key = st.text_input(label="confluence space",
                             help="Space of Confluence",
                             value="RD")
    api_key = st.text_input(label="confluence api key",
                            help="leave blank if confluence page is public",
                            type="password")
    submitted1 = st.form_submit_button(label='Submit')

    if submitted1 and confluence_url and space_key:
        st.session_state["config"] = {
            "persist_directory": None,
            "confluence_url": confluence_url,
            "username": username if username != "" else None,
            "api_key": api_key if api_key != "" else None,
            "space_key": space_key,
        }
        with st.spinner(text="Ingesting Confluence..."):
            ### Hardcoding for https://templates.atlassian.net/wiki/ and space RD to avoid multiple OpenAI calls.
            config = st.session_state["config"]
            if  config["confluence_url"] == "https://templates.atlassian.net/wiki/" and config["space_key"] =="RD":
                config["persist_directory"] = "chroma_db"
            st.session_state["config"] = config

            st.session_state["confluence_qa"]  = load_confluence(st.session_state["config"])
        st.write("Confluence Space Ingested")
        

st.title("Confluence Q&A Demo")

question = st.text_input('Ask a question', "How do I make a space public?")

if st.button('Get Answer', key='button2'):
    with st.spinner(text="Asking LLM..."):
        confluence_qa = st.session_state.get("confluence_qa")
        if confluence_qa is not None:
            result = confluence_qa.answer_confluence(question)
            st.write(result)
        else:
            st.write("Please load Confluence page first.")


confluence_qa.py
import os
from langchain.document_loaders import ConfluenceLoader
from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
from constants import *

from langchain.vectorstores import FAISS   # ⬅️ use FAISS
from langchain.chains import RetrievalQA
import pickle

class ConfluenceQA:
    def __init__(self, config: dict = {}):
        self.config = config
        self.embedding = None
        self.vectordb = None
        self.llm = None
        self.qa = None
        self.retriever = None

    def init_embeddings(self) -> None:
        # Azure OpenAI embeddings
        self.embedding = AzureOpenAIEmbeddings(
            deployment=os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT"),
            model="text-embedding-ada-002",
            chunk_size=1
        )

    def init_models(self) -> None:
        # Azure OpenAI GPT model
        self.llm = AzureChatOpenAI(
            deployment_name=os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT"),
            temperature=0.0
        )

    def vector_db_confluence_docs(self, force_reload: bool = False) -> None:
        persist_directory = self.config.get("persist_directory", None)
        faiss_index_path = os.path.join(persist_directory, "faiss_index.pkl") if persist_directory else None

        confluence_url = self.config.get("confluence_url", None)
        username = self.config.get("username", None)
        api_key = self.config.get("api_key", None)
        space_key = self.config.get("space_key", None)

        # Load FAISS index if available
        if faiss_index_path and os.path.exists(faiss_index_path) and not force_reload:
            with open(faiss_index_path, "rb") as f:
                self.vectordb = pickle.load(f)
        else:
            # 1. Extract the documents
            loader = ConfluenceLoader(url=confluence_url, username=username, api_key=api_key)
            documents = loader.load(space_key=space_key, limit=100)

            # 2. Split the texts
            text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)
            texts = text_splitter.split_documents(documents)
            text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=10, encoding_name="cl100k_base")
            texts = text_splitter.split_documents(texts)

            # 3. Create FAISS index
            self.vectordb = FAISS.from_documents(texts, self.embedding)

            # Save FAISS index if persist_directory provided
            if faiss_index_path:
                with open(faiss_index_path, "wb") as f:
                    pickle.dump(self.vectordb, f)

    def retreival_qa_chain(self):
        self.retriever = self.vectordb.as_retriever(search_kwargs={"k": 4})
        self.qa = RetrievalQA.from_chain_type(llm=self.llm, chain_type="stuff", retriever=self.retriever)

    def answer_confluence(self, question: str) -> str:
        return self.qa.run(question)

constants.py

# Constants
EMB_OPENAI_ADA = "text-embedding-ada-002"
EMB_SBERT = None # Chroma takes care

LLM_OPENAI_GPT35 = "gpt-3.5-turbo"

.env
AZURE_OPENAI_API_KEY=your-azure-key
AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com/
AZURE_OPENAI_API_VERSION=2024-06-01   # pick the version matching your resource
AZURE_OPENAI_CHAT_DEPLOYMENT=gpt-35-turbo   # deployment name in Azure
AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-ada-002

