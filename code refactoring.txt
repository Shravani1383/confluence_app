"""
confluence_tool.py

Single-file Confluence Search + Content Extraction Tool
- Clean architecture
- Testable
- Agent-ready
"""

import os
import sys
import json
import re
from dataclasses import dataclass
from typing import List, Dict, Optional
from urllib.parse import urlparse, unquote, quote_plus

import urllib3
from bs4 import BeautifulSoup
from requests.models import PreparedRequest

from integrations.rest_client import RestClient
from config import config as cfg


# ============================================================
# Global Config (centralized, no magic values)
# ============================================================

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

DOMAIN_USER = cfg.DOMAIN_USER
DOMAIN_PASSWORD = cfg.DOMAIN_PASSWORD
CONFLUENCE_BASE_URL = cfg.ALM_CONFLUENCE_BASE_URL.rstrip("/")
PROXIES = getattr(cfg, "proxies", None)


# ============================================================
# Models
# ============================================================

@dataclass
class ConfluencePageRef:
    base_url: str
    space_key: Optional[str]
    page_id: Optional[str]
    title: Optional[str]


@dataclass
class PageSearchResult:
    title: str
    space_key: str
    url: str


@dataclass
class PageContent:
    page_id: str
    space_key: str
    title: str
    url: str
    plain_text_chunks: List[str]
    tables: List[List[List[str]]]


# ============================================================
# URL Parsing (PURE / UNIT TESTABLE)
# ============================================================

_SPACE_RE = re.compile(r"/spaces/(?P<space>[^/]+)/pages/(?P<id>\d+)")
_DISPLAY_RE = re.compile(r"/display/(?P<space>[^/]+)/(?P<title>[^/?#]+)")


def parse_confluence_url(url: str) -> ConfluencePageRef:
    parsed = urlparse(url)
    base_url = f"{parsed.scheme}://{parsed.netloc}"
    path = parsed.path

    if m := _SPACE_RE.search(path):
        return ConfluencePageRef(
            base_url, m.group("space"), m.group("id"), None
        )

    if m := _DISPLAY_RE.search(path):
        return ConfluencePageRef(
            base_url,
            m.group("space"),
            None,
            unquote(m.group("title").replace("+", " "))
        )

    return ConfluencePageRef(base_url, None, None, None)


# ============================================================
# REST CLIENT FACTORY (MOCKABLE)
# ============================================================

def create_rest_client() -> RestClient:
    return RestClient(
        credentials=(DOMAIN_USER, DOMAIN_PASSWORD),
        proxies=PROXIES,
        cert_file=None,
        disable_ssl_check=False,
    )


# ============================================================
# SEARCH (CQL)
# ============================================================

def build_cql(keywords: List[str]) -> str:
    return " AND ".join([f'text ~ "{kw}"' for kw in keywords]) if keywords else ""


def search_confluence_pages(
    keywords: List[str],
    max_results: int = 10,
) -> List[PageSearchResult]:

    cql = build_cql(keywords)
    endpoint = f"{CONFLUENCE_BASE_URL}/rest/api/content/search"

    params = {
        "cql": cql,
        "limit": max_results,
        "expand": "space",
    }

    req = PreparedRequest()
    req.prepare_url(endpoint, params)

    client = create_rest_client()
    resp = client.get(req.url)

    results = []
    for page in resp.get("results", []):
        title = page.get("title", "")
        space_key = page.get("space", {}).get("key", "")
        title_encoded = quote_plus(title)

        url = (
            f"{CONFLUENCE_BASE_URL}/display/{space_key}/{title_encoded}"
            if space_key and title_encoded
            else ""
        )

        results.append(PageSearchResult(title, space_key, url))

    return results


# ============================================================
# FETCH PAGE CONTENT
# ============================================================

def build_content_endpoint(ref: ConfluencePageRef) -> str:
    if ref.page_id:
        return (
            f"{ref.base_url}/rest/api/content/{ref.page_id}"
            "?expand=body.storage,space,title"
        )

    if ref.space_key and ref.title:
        return (
            f"{ref.base_url}/rest/api/content"
            f"?spaceKey={ref.space_key}&title={quote_plus(ref.title)}"
            "&expand=body.storage,space,title"
        )

    raise ValueError("Insufficient URL data to fetch page")


def normalize_response(resp: Dict) -> Dict:
    return resp["results"][0] if "results" in resp and resp["results"] else resp


# ============================================================
# HTML â†’ STRUCTURED CONTENT
# ============================================================

def parse_page_html(page: Dict, url: str) -> PageContent:
    html = page.get("body", {}).get("storage", {}).get("value", "")
    soup = BeautifulSoup(html, "html.parser")

    tables = []
    for table in soup.find_all("table"):
        rows = []
        for tr in table.find_all("tr"):
            cells = [c.get_text(" ", strip=True) for c in tr.find_all(["th", "td"])]
            if cells:
                rows.append(cells)
        if rows:
            tables.append(rows)

    text_chunks = [
        tag.get_text(" ", strip=True)
        for tag in soup.find_all(["p", "h1", "h2", "h3", "h4", "li"])
        if tag.get_text(strip=True)
    ]

    return PageContent(
        page_id=page.get("id", ""),
        space_key=page.get("space", {}).get("key", ""),
        title=page.get("title", ""),
        url=url,
        plain_text_chunks=text_chunks,
        tables=tables,
    )


def fetch_confluence_page(url: str) -> Dict:
    ref = parse_confluence_url(url)
    endpoint = build_content_endpoint(ref)

    client = create_rest_client()
    resp = normalize_response(client.get(endpoint))

    content = parse_page_html(resp, url)
    return content.__dict__


# ============================================================
# AGENT ORCHESTRATOR (TOOL ENTRYPOINT)
# ============================================================

def confluence_agent_tool(
    keywords: List[str],
    max_results: int = 5,
) -> Dict:
    """
    Single agent-facing orchestration method
    """

    pages = search_confluence_pages(keywords, max_results)
    results = []

    for page in pages:
        try:
            content = fetch_confluence_page(page.url)
            results.append(content)
        except Exception as e:
            results.append({"url": page.url, "error": str(e)})

    return {
        "query": keywords,
        "results": results,
    }


# ============================================================
# CLI (OPTIONAL)
# ============================================================

if __name__ == "__main__":
    keywords = ["Productivity Suite Features"]
    data = confluence_agent_tool(keywords, max_results=3)

    with open("confluence_results.json", "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print("Saved results to confluence_results.json")
